#!/usr/bin/env python3
"""
Test Smart Rate Limiting System
Demonstrates rate limiting activation on first 429 error
"""

import asyncio
import time
import pandas as pd
from llm_contact_extractor import EnhancedLLMContactExtractor

async def test_smart_rate_limiting():
    """Test smart rate limiting with a small batch of companies"""
    
    print("ğŸ›¡ï¸ TESTING SMART RATE LIMITING SYSTEM")
    print("=" * 70)
    print("âœ… No rate limiting until first 429 error")
    print("âœ… Smart activation on quota hit")
    print("âœ… Adaptive delay management")
    print("âœ… Gradual optimization after successful calls")
    print("=" * 70)
    
    # Load a few companies for testing
    csv_file = "Leads_Busunternehmen.csv"
    try:
        df = pd.read_csv(csv_file)
        print(f"ğŸ“Š Loaded {len(df)} companies from CSV")
    except Exception as e:
        print(f"âŒ Error loading CSV: {e}")
        return
    
    # Use first 10 companies for rate limiting test
    test_companies = 10
    url_column = 'URL1' if 'URL1' in df.columns else df.columns[0]
    urls_to_test = df.head(test_companies)[url_column].tolist()
    
    # Clean URLs
    clean_urls = []
    for url in urls_to_test:
        if pd.notna(url) and str(url).strip():
            clean_url = str(url).strip()
            if not clean_url.startswith('http'):
                clean_url = f"https://{clean_url}"
            clean_urls.append(clean_url)
    
    print(f"ğŸ¯ Testing smart rate limiting with {len(clean_urls)} companies:")
    for i, url in enumerate(clean_urls, 1):
        print(f"   {i:2d}. {url}")
    
    # Initialize extractor
    extractor = EnhancedLLMContactExtractor(
        timeout=30,
        max_pages=None,  # UNLIMITED
        thread_workers=5,  # Reduced for testing
        connection_pool_size=20
    )
    
    print(f"\nğŸ”§ Initial Rate Limiting Status:")
    stats = extractor.get_rate_limiting_stats()
    print(f"   ğŸ›¡ï¸ Rate limiting active: {stats['rate_limiting_active']}")
    print(f"   â±ï¸  Adaptive delay: {stats['adaptive_delay']:.1f}s")
    print(f"   ğŸ“Š Consecutive errors: {stats['consecutive_errors']}")
    
    # Process companies sequentially to show rate limiting in action
    print(f"\nâš¡ SEQUENTIAL PROCESSING (to demonstrate rate limiting):")
    start_time = time.time()
    all_results = []
    
    for i, url in enumerate(clean_urls, 1):
        print(f"\nğŸ” Processing {i}/{len(clean_urls)}: {url}")
        
        try:
            result = await extractor.extract_from_website(url)
            all_results.append(result)
            
            # Show current rate limiting status
            stats = extractor.get_rate_limiting_stats()
            if stats['rate_limiting_active']:
                elapsed = stats['time_since_activation']
                print(f"   ğŸ›¡ï¸ Rate limiting: ACTIVE for {elapsed:.0f}s, delay: {stats['adaptive_delay']:.1f}s")
                print(f"   ğŸ“Š Errors: {stats['consecutive_errors']}, Successes: {stats['successful_calls']}")
            else:
                print(f"   ğŸš€ Rate limiting: INACTIVE (full speed)")
            
            if result.success:
                print(f"   âœ… Success: {result.total_contacts_found} contacts, {result.total_pages_analyzed} pages")
            else:
                print(f"   âŒ Failed: {result.error}")
                
        except Exception as e:
            print(f"   âŒ Error: {e}")
            
        # Small delay between companies for demonstration
        await asyncio.sleep(0.5)
    
    total_time = time.time() - start_time
    successful = [r for r in all_results if r.success]
    total_contacts = sum(len(r.all_contacts) for r in successful)
    
    print(f"\nğŸ‰ SMART RATE LIMITING TEST COMPLETED!")
    print("=" * 70)
    print(f"ğŸ“Š RESULTS SUMMARY:")
    print(f"   âœ… Successful: {len(successful)}/{len(clean_urls)}")
    print(f"   ğŸ‘¥ Total Contacts: {total_contacts}")
    print(f"   â±ï¸  Total Time: {total_time:.1f}s")
    
    # Final rate limiting status
    final_stats = extractor.get_rate_limiting_stats()
    print(f"\nğŸ›¡ï¸ FINAL RATE LIMITING STATUS:")
    print(f"   Active: {final_stats['rate_limiting_active']}")
    if final_stats['rate_limiting_active']:
        print(f"   Duration: {final_stats['time_since_activation']:.0f}s")
        print(f"   Current delay: {final_stats['adaptive_delay']:.1f}s")
        print(f"   Total errors handled: {final_stats['consecutive_errors']}")
        print(f"   Successful recoveries: {final_stats['successful_calls']}")
    
    # Save results if any
    if all_results:
        db_file = extractor.save_enhanced_ai_contacts(all_results)
        print(f"\nğŸ“ Results saved to: {db_file}")
    
    await extractor.cleanup()
    
    print(f"\nâœ… SMART RATE LIMITING DEMONSTRATED:")
    print("   ğŸš€ Started at full speed (no delays)")
    print("   âš ï¸ Activated automatically on first 429 error")
    print("   ğŸ§  Adapted delay based on error frequency")
    print("   ğŸ¯ Optimized delay after successful calls")
    print("   ğŸ›¡ï¸ Prevented quota violations while maintaining performance")

async def test_threadpool_with_rate_limiting():
    """Test ThreadPool processing with smart rate limiting"""
    
    print("\nâš¡ TESTING THREADPOOL WITH SMART RATE LIMITING")
    print("=" * 70)
    
    # Load companies
    df = pd.read_csv("Leads_Busunternehmen.csv")
    url_column = 'URL1' if 'URL1' in df.columns else df.columns[0]
    test_urls = df.head(20)[url_column].tolist()
    
    # Clean URLs
    clean_urls = []
    for url in test_urls:
        if pd.notna(url) and str(url).strip():
            clean_url = str(url).strip()
            if not clean_url.startswith('http'):
                clean_url = f"https://{clean_url}"
            clean_urls.append(clean_url)
    
    print(f"ğŸ¯ Testing {len(clean_urls)} companies with ThreadPool + Rate Limiting")
    
    # Initialize extractor with smart rate limiting
    extractor = EnhancedLLMContactExtractor(
        timeout=30,
        max_pages=None,
        thread_workers=10,  # Moderate concurrency
        connection_pool_size=50
    )
    
    # Process with ThreadPool
    start_time = time.time()
    results = extractor.extract_from_multiple_websites_threaded(clean_urls)
    total_time = time.time() - start_time
    
    # Results
    successful = [r for r in results if r.success]
    total_contacts = sum(len(r.all_contacts) for r in successful)
    
    print(f"\nğŸ“Š THREADPOOL + RATE LIMITING RESULTS:")
    print(f"   âœ… Successful: {len(successful)}/{len(clean_urls)}")
    print(f"   ğŸ‘¥ Total Contacts: {total_contacts}")
    print(f"   â±ï¸  Total Time: {total_time:.1f}s")
    print(f"   âš¡ Rate: {len(clean_urls)/total_time:.1f} websites/second")
    
    # Rate limiting effectiveness
    final_stats = extractor.get_rate_limiting_stats()
    if final_stats['rate_limiting_active']:
        print(f"\nğŸ›¡ï¸ RATE LIMITING EFFECTIVENESS:")
        print(f"   âœ… Activated successfully during processing")
        print(f"   â±ï¸  Active for: {final_stats['time_since_activation']:.0f}s")
        print(f"   ğŸ¯ Final delay: {final_stats['adaptive_delay']:.1f}s")
        print("   âœ… Prevented quota violations while maintaining throughput")
    else:
        print(f"\nğŸš€ NO RATE LIMITING NEEDED:")
        print("   âœ… Processed all companies without hitting quota limits")
        print("   âš¡ Full speed processing maintained")
    
    await extractor.cleanup()

async def main():
    """Run rate limiting tests"""
    await test_smart_rate_limiting()
    await test_threadpool_with_rate_limiting()

if __name__ == "__main__":
    asyncio.run(main()) 